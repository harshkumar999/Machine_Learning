{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMG3LTMytzR/WkRLwaNmdzK",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/harshkumar999/Machine_Learning/blob/main/Linear_Regression.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Machine Learning**\n",
        "\n",
        "- Supervised ML\n",
        "- Unsupervised ML"
      ],
      "metadata": {
        "id": "ttYldeazHIYL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Supervised Machine Learning**\n",
        "\n",
        "* Supervised ML is used when a dataset has a target variable (also called a dependent variable).\n",
        "\n",
        "* **Dependent** variable is what we want to predict, while **independent** variables (or features) are used to make predictions about the target variables."
      ],
      "metadata": {
        "id": "mOlSfSSzg6TM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Supervised ML**\n",
        "\n",
        "* Regression\n",
        "* Classification"
      ],
      "metadata": {
        "id": "Xj5sXrpnhwJt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Regression**\n",
        "\n",
        "- When the target variable has continous values, we use **regression**, such as predicting house prices.\n",
        "\n",
        "**Classification**\n",
        "\n",
        "- When the target variable is divided into categories, we use **classification**, like determining if an email is spam or not."
      ],
      "metadata": {
        "id": "ebvFWdkAiQ36"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Regression**: It is the relationship b/w two variables."
      ],
      "metadata": {
        "id": "5DYEIvc5i1of"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Linear Regression**\n",
        "\n",
        "- It is the relationship b/w the target variable and each independent variables.\n",
        "\n",
        "- Linear regression is based on an equation of line.\n",
        "\n",
        "          y = mx + c\n",
        "\n",
        "**y** = Predicted value\n",
        "\n",
        "**x** = Input value\n",
        "\n",
        "**m** = slope of the line (weight) : It indicates how much y changes when there is a change in x. It is also known as the coefficient.\n",
        "\n",
        "**c** = y-intercept point : It is the value of y when x=0. It represents the point where the line crosses the y-axis.\n",
        "\n",
        "**parameters** : m and c\n",
        "\n",
        "      Input -> Model -> Output\n"
      ],
      "metadata": {
        "id": "17JKPIXRi9Rm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Simple Linear Regression**: One independent variable.\n",
        "\n",
        "      h(θ) = θo + θ1x\n",
        "\n",
        "**Multiple Linear Regression**: Multiple independent variables.\n",
        "\n",
        "      h(θ) = θo + θ1x1 + θ2x2 + ... + θnxn"
      ],
      "metadata": {
        "id": "QJY2tUjIlaA2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- We need to make a best-fit line to predict our outcomes.\n",
        "\n",
        "- We always try to reduce error/residual."
      ],
      "metadata": {
        "id": "PPmsZbY_mVFp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![alt](data:image/png;base64,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)"
      ],
      "metadata": {
        "id": "oEiRAKrLmexh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Error/Residual**: It is the difference b/w actual value and predicted value.\n",
        "\n",
        "**Cost Function**: It measures the average squared difference b/w actual value and predicted value."
      ],
      "metadata": {
        "id": "bqrAecGJq1Gv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src=\"https://www.oreilly.com/api/v2/epubs/9781788837996/files/assets/39a93548-a5c8-42d4-8696-f1cac167f5b9.png\" alt=\"Description\" height=\"100\">\n"
      ],
      "metadata": {
        "id": "c0OIBX8kr5up"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**y^** = Predicted value\n",
        "\n",
        "**y** = Actual value\n",
        "\n",
        "**m** = number of datapoints\n"
      ],
      "metadata": {
        "id": "CSypdxbQsul7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Cost Function** is the average error across the entire dataset, while **loss function** is the error for a single datapoint."
      ],
      "metadata": {
        "id": "KV-6IVzuMgM2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Model Optimization**\n",
        "\n",
        "Optimization refers to determining best parameters for a model, such that the loss function of the model decreases, as a result of which the model can predict more accurately."
      ],
      "metadata": {
        "id": "ULKnp-ees-vY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Gradient Descent**\n",
        "\n",
        "- Gradient Descent is an optimization algorithm used for minimizing the cost function in various machine learning algorithm.\n",
        "\n",
        "- It is used for updating the parameters of the learning model.\n",
        "\n",
        "      m = m - α Dm\n",
        "      c = c - α Dc\n",
        "\n",
        "**m** : slope\n",
        "\n",
        "**c** : intercept\n",
        "\n",
        "**α** : learning rate\n",
        "\n",
        "**Dm** : partial derivate of cost function with respect to m\n",
        "\n",
        "**Dc** : partial derivate of cost function with respect to c\n",
        "      \n",
        "      Dm = ∂(cost function) / ∂m\n",
        "      Dc = ∂(cost function) / ∂c\n",
        "\n",
        "Best Learning Rate (α) = 0.001"
      ],
      "metadata": {
        "id": "mjDuFspCtYIS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src=\"https://miro.medium.com/v2/resize:fit:1154/0*XwAiiRFSgXyvOX7e\" height=\"200\">"
      ],
      "metadata": {
        "id": "AYWSEXuTPU_n"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Global minima** is the point where the cost function has the lowest value overall, while the **local minima** is where the cost function has the lowest value in a specific region but not necessarily the lowest overall."
      ],
      "metadata": {
        "id": "IN1JgATAPs-T"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Performance Metrics of Linear Regression**\n",
        "\n",
        "**MSE** (Mean Squared Error) : It measures the average squared difference b/w actual and predicted values.\n",
        "\n",
        "      MSE = Cost Function\n",
        "\n",
        "      MSE ↑  Bad Model\n",
        "      MSE ↓  Good Model\n",
        "\n",
        "\n",
        "**RMSE** (Root Mean Squared Error) : It is the square root of MSE giving error in the same units as the target variable.\n",
        "\n",
        "- Easy to understand compared to MSE beacuse it is in the same units as the data.\n",
        "\n",
        "        RMSE = √MSE\n",
        "\n",
        "**R-Squared** (R²)\n",
        "\n",
        "            R² = 1  - (SSR / SST)\n",
        "\n",
        "SSR : Sum of square of residuals (difference b/w actual and predicted values).\n",
        "\n",
        "SST : Total sum of squares (difference b/w actual values and their mean).\n"
      ],
      "metadata": {
        "id": "LHcnK8EEQET8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src=\"https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcTKSqCQcwhJRwP4d7RR4xTpwFHeiBKeLpbHFg&s\">"
      ],
      "metadata": {
        "id": "B39QzCd1Sf9A"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "      Range: 0 to 1\n",
        "\n",
        "      If R² is more that means model is good.\n",
        "      If R² is less that means model is bad.\n",
        "\n",
        "      Important: R² can be high even if you add irrelevant features so it is not always reliable.\n",
        "\n"
      ],
      "metadata": {
        "id": "dbAYIOo2Stos"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Adjusted R²**\n",
        "\n",
        "<img src= \"https://miro.medium.com/v2/resize:fit:1030/1*xpt5i5JVzZQ454WO10LQ4w.png\" height=\"150\">\n",
        "\n",
        "        N = number of observations (datapoints)\n",
        "        P =  number of predictors (features) used in the model.\n",
        "\n",
        "- Value of Adjusted R² is always less than or equal to R².\n",
        "\n",
        "- Adjusted R² is similar to R², but it takes into account how many features (variables) you’re using in your model. If you add too many irrelevant features, adjusted R² will lower the score, helping you see if your model is really doing a good job."
      ],
      "metadata": {
        "id": "tGuWaXVjTJeD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Overfitting** : When a model has high accuracy on training data but poor performance on testing data, it typically indicates overfitting.\n",
        "\n",
        "**Underfitting**: When a model has poor performance on both training and testing data, it typically indicates underfitting.\n",
        "\n",
        "**Bias** : Difference b/w predicted and actual values.\n",
        "\n",
        "      High Bias -> Large differnce -> Underfitting\n",
        "\n",
        "**Variance** : Variability in model predictions for a given data point, reflecting how much the predictions change with change in training data.\n",
        "\n",
        "- High Variance means the model is too sensitive to the training data.\n",
        "\n",
        "      High Variance -> Overfitting\n",
        "\n",
        "\n",
        "**Generalized Model**: Low Bias and Low Variance (Ideal Model).\n",
        "\n",
        "**Conditions for Overfiiting:**\n",
        "\n",
        "        - Large training dataset.\n",
        "        - Complex dataset that makes it difficult to find patterns.\n",
        "        - Too many irrelevant features.\n",
        "\n",
        "**How to Overcome Overfitting:**\n",
        "\n",
        "        - Simplify the model.\n",
        "        - Reduce irrelevant features.\n",
        "        - Eliminate noise and outliers from the dataset.\n",
        "\n",
        "\n",
        "**Conditions for Underfitting:**\n",
        "\n",
        "        - Model is too simple.\n",
        "        - Very small training dataset.\n",
        "        - Insufficient features.\n",
        "\n",
        "**How to Overcome Underfitting:**\n",
        "\n",
        "        - Increase model complexity.\n",
        "        - Increase the training dataset.\n",
        "        - Add more relevant features."
      ],
      "metadata": {
        "id": "6CMOcYLsVCbe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Bias-Variance Trade-off**\n",
        "\n",
        "<img src=\"https://serokell.io/files/y0/y08hu0d1.Bias-Variance_Tradeoff_in_ML_pic6.png\" height=\"400\">\n",
        "\n",
        "          Overfitting Model: Low Bias\n",
        "                             High Variance\n",
        "\n",
        "          Underfitting Model: High Bias\n",
        "                              Low Variance\n",
        "      \n",
        "          Generalized Model:  Low Bias\n",
        "                              Low Variance"
      ],
      "metadata": {
        "id": "9s8z4bGKaDQD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Regularization**\n",
        "\n",
        "- Regularization prevents overfiiting by keeping models simpler and more generlizable, ensuring better performance on new data.\n",
        "\n",
        "- Overfitting happens when a model learns the training data too well, including its noise and outliers, making it perform poorly on new data."
      ],
      "metadata": {
        "id": "gltXSeEFawG0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Regularization Techniques**\n",
        "\n",
        "**Lasso** (L1 Regularization)\n",
        "\n",
        "        Formula: cost function = cost function + λ |slope|\n",
        "\n",
        "- It helps in feature selection by eliminating less important features by setting their coefficients to zero."
      ],
      "metadata": {
        "id": "KjLddbt7bqvU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Ridge Regression**(L2 Regularization)\n",
        "\n",
        "        Formula: cost function = cost function + λ (slope)²\n",
        "\n",
        "- It keeps all features and reduces the impact of highly correlated features by making their coefficients smaller."
      ],
      "metadata": {
        "id": "Iu0axLgOc50h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Elastic Net**\n",
        "\n",
        "        Formula: cost function = cost function + λ |slope| + λ (slope)²\n",
        "    \n",
        "- It combines L1 and L2 regularization. It is useful when there are multiple features correlated with each other. It helps in feature selectionn while maintaining regularization to handle multicollinearity."
      ],
      "metadata": {
        "id": "_jXrwHjyem6K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**λ** is the regularization parameter that controls how much we reduce the model's complexity to prevent overfitting.\n",
        "\n",
        "- It controls the strength of regularization applied to a model, balancing complexity and performance."
      ],
      "metadata": {
        "id": "gV2l1_znf7uZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- **Paramters** are values learned by the model during training (e.g., weights).\n",
        "\n",
        "- **Hyperparamters** are settings choose before training a model. (e.g., learning rate)."
      ],
      "metadata": {
        "id": "pXr24jOggl1p"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Cross Validation**(CV): It is a technique to find the best hyperparameters by evaluating model performance on different subsets of the data.\n",
        "\n",
        "- It helps avoid overfittig and gives a better idea of model accuracy."
      ],
      "metadata": {
        "id": "fBK5EwP_hKa4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Grid Search CV**\n",
        "\n",
        "      - It is primarily used with small datasets to find the best hyperparameters for your model.\n",
        "\n",
        "      - It tests every possible combinations of given hyperparameters.\n",
        "\n",
        "      - It ensures you explore all options to find the best performance.\n",
        "\n",
        "**Random Search CV**\n",
        "\n",
        "      - It is mainly used with large datasets to quickly find the best hyperparameters for your model.\n",
        "\n",
        "      - It tests a random selection of hyperparameters combinations instead of all.\n",
        "\n",
        "      - It saves times and often find best hyperparameters withour testing everything.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "9EwmZ-N7hrHL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Assumption of Linear Regression**\n",
        "\n",
        "**1. Linear Relationship**: There should be a linear relationship b/w the input (indepedent variables) and the output (dependent variable).\n",
        "\n",
        "<img src=\"https://www.emathzone.com/wp-content/uploads/2014/10/linear-nonlinear-corrrelation.jpg\">\n",
        "\n",
        "**2. No Multicollinearity**: Independent variables (inputs) should not be highly correlated with each others.\n",
        "\n",
        "- If you have there features (x1, x2, x3) predicting y, they should be independent. For example, if x1 changes, the effect on y should be clear, no matters what the values of x2 and x3 are.\n",
        "\n",
        "**3. Normality of Residual**: Errors (difference b/w predicted and actual values) should look like a bell curve when you plot them.\n",
        "\n",
        "- This means they follow a normal distribution.\n",
        "\n",
        "**4. Homoscedasticity**: The Spead of errors should be the same across all level of x.\n",
        "\n",
        "- This means the errors should not get wider or narrow as x changes.\n",
        "\n",
        "<img src=\"https://lh3.googleusercontent.com/B2hT0jQlT2xw6-pRWlqMktNDhiteFjk32W13_stPWUU72uaMOxIKGDqhGOzS1x48rl1vMWF72x08x34xnuHueiJ2YcQZHqTpT9jYU_iENLlV9RfJ5nAaWOELMOUEUJJ1ATkJ1E01z6mpI0Ko\">\n",
        "\n",
        "<img src=\"https://miro.medium.com/v2/resize:fit:638/1*OnODBB93DWEr_sZoVjZS4A.png\">\n",
        "\n",
        "\n",
        "**5. No Autocorrelation of Errors**: The errors should not be correlated with each other.\n",
        "\n",
        "- This means that the error for one observation should not predict the error for another observation.\n",
        "\n",
        "- This is particularly important in time series data.\n",
        "\n",
        "<img src=\"https://miro.medium.com/v2/resize:fit:1400/1*e-O6ADdrNW6yyPzu8RL7Ww.png\" height=\"150\">"
      ],
      "metadata": {
        "id": "eDeti5Rpi8fr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **VIF** (Variance Inflation Factor)\n",
        "\n",
        "- It is used to detect mutlicollinarity, which happens when two or more independent variables are highly correlated.\n",
        "\n",
        "      Formula:  VIF = 1 / (1 - R²)\n",
        "  \n",
        "R² is the coefficient of determination in linear regression. Its value lies between 0 and 1.\n",
        "\n",
        "**Example**: The dataset contains height, weight, gender ans BMI.\n",
        "\n",
        "      Gender  Height  Weight  Index\n",
        "      Male     174      96      4    \n",
        "      Male     189      87      2\n",
        "      Female   185     110      4\n",
        "      Female   195     104      3\n",
        "      Male     149      61      3\n",
        "\n",
        "\n",
        "       feature     VIF\n",
        "      Gender   2.028864\n",
        "      Height  11.623103\n",
        "      Weight  10.688377\n",
        "\n",
        "As we can see, height and weight have very high values of VIF, indicating that these two variables are highly correlated. This is expected as the height of a person does influence their weight. Hence, considering these two features together leads to a model with high multicollinearity.\n",
        "\n",
        "VIF > 5 indicates high multicollinearity."
      ],
      "metadata": {
        "id": "2qDQt3a4nJHq"
      }
    }
  ]
}